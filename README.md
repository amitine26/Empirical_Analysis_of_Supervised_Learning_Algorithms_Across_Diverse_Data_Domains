# Empirical Analysis of Supervised Learning Algorithms Across Diverse Data Domains

In this study, I evaluated the performance of five supervised learning classifiers, 
Random Forest, XGBoost, Support Vector Machines (SVM), Logistic Regression, and K-Nearest 
Neighbors (KNN), across four datasets of varying dimensionality and domain (Adult, Letter 
Recognition, Spambase, and Mushroom). Following the experiment by Caruana and Niculescu-Mizil, 
I analyzed the impact of training set size on generalization error. My results demonstrated 
that ensemble methods (XGBoost and Random Forest) consistently outperform single estimators 
on complex tabular data, while geometric methods (SVM and KNN) excel in spatial domains like 
character recognition. Furthermore, I confirmed that increasing training data volume yields 
diminishing returns, with the most significant gains observed in lower-data regimes.

Link to Project File: https://github.com/amitine26/Empirical_Analysis_of_Supervised_Learning_Algorithms_Across_Diverse_Data_Domains/blob/main/COGS118A_Final_Project.ipynb

Link to Report: https://drive.google.com/file/d/1PMA40ckfMYAHLKYzF6gttpJsSe3N4vKU/view?usp=sharing
