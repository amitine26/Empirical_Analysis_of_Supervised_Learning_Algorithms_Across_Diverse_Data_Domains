{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fa0b11-d7c7-4159-9bf7-e7ee685b194d",
   "metadata": {},
   "source": [
    "# Empirical Analysis of Supervised Learning Algorithms Across Diverse Data Domains #\n",
    "**Author:** Anthony Mitine \\\n",
    "**Course:** COGS 118A. Supervised Machine Learning Algorithms\\\n",
    "**Date:** December 11, 2025 \n",
    "\n",
    "## Overview ##\n",
    "\n",
    "In this study, I empirically evaluated the performance of five distinct supervised learning classifiers, Random Forest, XGBoost, Support Vector Machines (SVM), Logistic Regression, and K-Nearest Neighbors (KNN), across four datasets of varying dimensionality and domain (Adult, Letter Recognition, Spambase, and Mushroom). Following the experimental protocols established by Caruana and Niculescu-Mizil, I analyzed the impact of training set size on generalization error. My results demonstrated that ensemble methods (XGBoost and Random Forest) consistently outperform single estimators on complex tabular data, while geometric methods (SVM and KNN) excel in spatial domains like character recognition. Furthermore, I confirmed that increasing training data volume yields diminishing returns, with the most significant gains observed in lower-data regimes.\n",
    "\n",
    "**Link to Report**: https://drive.google.com/file/d/1PMA40ckfMYAHLKYzF6gttpJsSe3N4vKU/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe55b2a6-0b33-4692-b98a-d29d4e6c90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all needed libraries\n",
    "import warnings\n",
    "# filter out the specific threadpoolctl warning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"threadpoolctl\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978b9851-29b9-4474-af35-ea12ba616cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all needed classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3135e26-bbd7-4ef7-a186-b909140b8023",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Here we define the experimental hyperparameters. We select four distinct datasets from the UCI repository (Adult, Letter, Spambase, Mushroom) to ensure diversity in problem domain. We also initialize the five classifiers (Random Forest, XGBoost, SVM, Logistic Regression, KNN) and define their hyperparameter search grids for the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb4ad2b-e13f-40a3-8e20-38bfe42504de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 4 datasets to use (Name, UCI ID)\n",
    "\n",
    "datasets_config = {\n",
    "    'adult': 2,       \n",
    "    'letter': 59,     \n",
    "    'spambase': 94,   \n",
    "    'mushroom': 73    \n",
    "}\n",
    "\n",
    "# Partitions and Trials\n",
    "partitions = [0.2, 0.5, 0.8] \n",
    "num_trials = 3 \n",
    "\n",
    "# 5 classifiers \n",
    "model_param_grids = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100],\n",
    "            'classifier__max_depth': [None, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 10]\n",
    "        }\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': XGBClassifier(eval_metric='logloss', n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100],\n",
    "            'classifier__learning_rate': [0.1, 0.3],\n",
    "            'classifier__max_depth': [3, 6]\n",
    "        }\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(random_state=42, cache_size=1000),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__kernel': ['rbf'] \n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'knn': {\n",
    "        'model': KNeighborsClassifier(n_jobs=-1),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 9],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e72b1-a988-4e27-b470-2d0e7bf47261",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "This function handles the extract, transform, and load process. It fetches datasets dynamically using the ucimlrepo API and applies dataset-specific preprocessing. This includes converting multi-class targets into binary classification problems (e.g., Letter 'A' vs. Not 'A') and cleaning missing values to prepare the feature matrices for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28891a12-08fb-4a4f-bdb4-760be5c50f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading   \n",
    "\n",
    "def load_and_prep_data(name, uci_id):\n",
    "    print(f\"\\n[data load] fetching {name} (id: {uci_id})...\")\n",
    "    \n",
    "    try:\n",
    "        dataset = fetch_ucirepo(id=uci_id)\n",
    "        x = dataset.data.features\n",
    "        y = dataset.data.targets\n",
    "\n",
    "        # Preprocessing logic \n",
    "        \n",
    "        # 1. Adult dataset \n",
    "        if name == 'adult':\n",
    "            y = y.iloc[:, 0].astype(str) \n",
    "            y = y.apply(lambda val: 1 if '>50K' in val else 0)\n",
    "            combined = pd.concat([x, y], axis=1).dropna()\n",
    "            combined = combined.replace('?', np.nan).dropna()\n",
    "            if len(combined) > 10000:\n",
    "                 combined = combined.sample(10000, random_state=42)\n",
    "            x = combined.iloc[:, :-1]\n",
    "            y = combined.iloc[:, -1]\n",
    "\n",
    "       # 2. Letter recognition \n",
    "        elif name == 'letter':\n",
    "            y = y.iloc[:, 0]\n",
    "            y = (y == 'A').astype(int)\n",
    "\n",
    "        # 3. Spambase \n",
    "        elif name == 'spambase':\n",
    "            y = y.iloc[:, 0]\n",
    "            # Ensure no nans\n",
    "            combined = pd.concat([x, y], axis=1).dropna()\n",
    "            x = combined.iloc[:, :-1]\n",
    "            y = combined.iloc[:, -1]\n",
    "\n",
    "        # 4. Mushroom \n",
    "        elif name == 'mushroom':\n",
    "            y = y.iloc[:, 0]\n",
    "            # target is 'e' (edible) or 'p' (poisonous). \n",
    "            y = (y == 'p').astype(int)\n",
    "            \n",
    "            combined = pd.concat([x, y], axis=1).replace('?', np.nan).dropna()\n",
    "            x = combined.iloc[:, :-1]\n",
    "            y = combined.iloc[:, -1]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error loading {name}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebe212-ed30-4e56-b774-1e9aee37273d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "\n",
    "I define a function to build Scikit-Learn pipelines. This combines preprocessing steps with the chosen classifier. This ensures that transformations are learned only from the training fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed8bfcf-de2d-4bae-887b-4573802b2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline helper function\n",
    "\n",
    "def get_pipeline(model_name, x):\n",
    "    \"\"\"\n",
    "    creates a scikit-learn pipeline with preprocessing + classifier.\n",
    "    \"\"\"\n",
    "    # Identify categorical and numerical columns\n",
    "    cat_cols = x.select_dtypes(include=['object', 'category']).columns\n",
    "    num_cols = x.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    # Preprocessor: scale numeric data, encode categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine preprocessor with the specific classifier\n",
    "    clf = model_param_grids[model_name]['model']\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affd428-4c36-45d9-872c-cb6d6130e60f",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "The main loop iterates through all combinations of datasets, classifiers, and data partitions (20%, 50%, 80% training splits). For each combination, it performs 3-fold cross-validation to tune hyperparameters, trains the optimal model, and records the average training and testing accuracy over multiple trials to ensure statistical reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c56e77-b556-4c42-ad5e-f7d16adba963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[data load] fetching adult (id: 2)...\n",
      "  dataset shape: (10000, 14)\n",
      "\n",
      "  --- partition: train 20% / test 80% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.8506 | avg train acc: 0.9185\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.8549 | avg train acc: 0.8758\n",
      "    running svm...\n",
      "      -> avg test acc: 0.8443 | avg train acc: 0.8727\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.8453 | avg train acc: 0.8517\n",
      "    running knn...\n",
      "      -> avg test acc: 0.8196 | avg train acc: 0.9998\n",
      "\n",
      "  --- partition: train 50% / test 50% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.8598 | avg train acc: 0.9099\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.8620 | avg train acc: 0.8797\n",
      "    running svm...\n",
      "      -> avg test acc: 0.8511 | avg train acc: 0.8727\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.8486 | avg train acc: 0.8560\n",
      "    running knn...\n",
      "      -> avg test acc: 0.8269 | avg train acc: 0.9999\n",
      "\n",
      "  --- partition: train 80% / test 19% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.8582 | avg train acc: 0.9089\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.8670 | avg train acc: 0.8833\n",
      "    running svm...\n",
      "      -> avg test acc: 0.8558 | avg train acc: 0.8724\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.8520 | avg train acc: 0.8553\n",
      "    running knn...\n",
      "      -> avg test acc: 0.8313 | avg train acc: 0.9999\n",
      "\n",
      "[data load] fetching letter (id: 59)...\n",
      "  dataset shape: (20000, 16)\n",
      "\n",
      "  --- partition: train 20% / test 80% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9940 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9963 | avg train acc: 1.0000\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9969 | avg train acc: 0.9997\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9908 | avg train acc: 0.9926\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9962 | avg train acc: 0.9990\n",
      "\n",
      "  --- partition: train 50% / test 50% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9961 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9976 | avg train acc: 1.0000\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9990 | avg train acc: 0.9998\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9906 | avg train acc: 0.9911\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9980 | avg train acc: 0.9996\n",
      "\n",
      "  --- partition: train 80% / test 19% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9972 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9980 | avg train acc: 1.0000\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9993 | avg train acc: 0.9998\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9903 | avg train acc: 0.9909\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9990 | avg train acc: 1.0000\n",
      "\n",
      "[data load] fetching spambase (id: 94)...\n",
      "  dataset shape: (4601, 57)\n",
      "\n",
      "  --- partition: train 20% / test 80% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9402 | avg train acc: 0.9993\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9356 | avg train acc: 0.9793\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9125 | avg train acc: 0.9587\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9058 | avg train acc: 0.9207\n",
      "    running knn...\n",
      "      -> avg test acc: 0.8833 | avg train acc: 1.0000\n",
      "\n",
      "  --- partition: train 50% / test 50% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9494 | avg train acc: 0.9986\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9483 | avg train acc: 0.9872\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9266 | avg train acc: 0.9714\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9189 | avg train acc: 0.9333\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9155 | avg train acc: 0.9996\n",
      "\n",
      "  --- partition: train 80% / test 19% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9501 | avg train acc: 0.9964\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9490 | avg train acc: 0.9812\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9312 | avg train acc: 0.9681\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9211 | avg train acc: 0.9308\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9218 | avg train acc: 0.9996\n",
      "\n",
      "[data load] fetching mushroom (id: 73)...\n",
      "  dataset shape: (5644, 22)\n",
      "\n",
      "  --- partition: train 20% / test 80% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 0.9994 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 0.9989 | avg train acc: 0.9997\n",
      "    running svm...\n",
      "      -> avg test acc: 0.9991 | avg train acc: 1.0000\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 0.9994 | avg train acc: 1.0000\n",
      "    running knn...\n",
      "      -> avg test acc: 0.9983 | avg train acc: 0.9994\n",
      "\n",
      "  --- partition: train 50% / test 50% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running svm...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running knn...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "\n",
      "  --- partition: train 80% / test 19% ---\n",
      "    running random_forest...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running xgboost...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running svm...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running logistic_regression...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "    running knn...\n",
      "      -> avg test acc: 1.0000 | avg train acc: 1.0000\n",
      "\n",
      "============================================================\n",
      "final results summary\n",
      "============================================================\n",
      " dataset  partition_train_pct          classifier  avg_train_acc  avg_test_acc\n",
      "   adult                 20.0                 knn       0.999833      0.819583\n",
      "   adult                 20.0 logistic_regression       0.851667      0.845333\n",
      "   adult                 20.0       random_forest       0.918500      0.850583\n",
      "   adult                 20.0                 svm       0.872667      0.844333\n",
      "   adult                 20.0             xgboost       0.875833      0.854917\n",
      "   adult                 50.0                 knn       0.999867      0.826867\n",
      "   adult                 50.0 logistic_regression       0.856000      0.848600\n",
      "   adult                 50.0       random_forest       0.909933      0.859800\n",
      "   adult                 50.0                 svm       0.872667      0.851133\n",
      "   adult                 50.0             xgboost       0.879733      0.862000\n",
      "   adult                 80.0                 knn       0.999917      0.831333\n",
      "   adult                 80.0 logistic_regression       0.855333      0.852000\n",
      "   adult                 80.0       random_forest       0.908917      0.858167\n",
      "   adult                 80.0                 svm       0.872375      0.855833\n",
      "   adult                 80.0             xgboost       0.883333      0.867000\n",
      "  letter                 20.0                 knn       0.999000      0.996167\n",
      "  letter                 20.0 logistic_regression       0.992583      0.990792\n",
      "  letter                 20.0       random_forest       1.000000      0.994021\n",
      "  letter                 20.0                 svm       0.999667      0.996875\n",
      "  letter                 20.0             xgboost       1.000000      0.996313\n",
      "  letter                 50.0                 knn       0.999633      0.998000\n",
      "  letter                 50.0 logistic_regression       0.991133      0.990600\n",
      "  letter                 50.0       random_forest       1.000000      0.996133\n",
      "  letter                 50.0                 svm       0.999800      0.999033\n",
      "  letter                 50.0             xgboost       1.000000      0.997567\n",
      "  letter                 80.0                 knn       1.000000      0.999000\n",
      "  letter                 80.0 logistic_regression       0.990875      0.990333\n",
      "  letter                 80.0       random_forest       1.000000      0.997167\n",
      "  letter                 80.0                 svm       0.999812      0.999333\n",
      "  letter                 80.0             xgboost       0.999958      0.998000\n",
      "mushroom                 20.0                 knn       0.999409      0.998302\n",
      "mushroom                 20.0 logistic_regression       1.000000      0.999410\n",
      "mushroom                 20.0       random_forest       1.000000      0.999410\n",
      "mushroom                 20.0                 svm       1.000000      0.999114\n",
      "mushroom                 20.0             xgboost       0.999704      0.998893\n",
      "mushroom                 50.0                 knn       1.000000      1.000000\n",
      "mushroom                 50.0 logistic_regression       1.000000      1.000000\n",
      "mushroom                 50.0       random_forest       1.000000      1.000000\n",
      "mushroom                 50.0                 svm       1.000000      1.000000\n",
      "mushroom                 50.0             xgboost       1.000000      1.000000\n",
      "mushroom                 80.0                 knn       1.000000      1.000000\n",
      "mushroom                 80.0 logistic_regression       1.000000      1.000000\n",
      "mushroom                 80.0       random_forest       1.000000      1.000000\n",
      "mushroom                 80.0                 svm       1.000000      1.000000\n",
      "mushroom                 80.0             xgboost       1.000000      1.000000\n",
      "spambase                 20.0                 knn       1.000000      0.883274\n",
      "spambase                 20.0 logistic_regression       0.920652      0.905823\n",
      "spambase                 20.0       random_forest       0.999275      0.940234\n",
      "spambase                 20.0                 svm       0.958696      0.912524\n",
      "spambase                 20.0             xgboost       0.979348      0.935615\n",
      "spambase                 50.0                 knn       0.999565      0.915544\n",
      "spambase                 50.0 logistic_regression       0.933333      0.918876\n",
      "spambase                 50.0       random_forest       0.998551      0.949442\n",
      "spambase                 50.0                 svm       0.971449      0.926554\n",
      "spambase                 50.0             xgboost       0.987246      0.948283\n",
      "spambase                 80.0                 knn       0.999638      0.921824\n",
      "spambase                 80.0 logistic_regression       0.930797      0.921100\n",
      "spambase                 80.0       random_forest       0.996377      0.950054\n",
      "spambase                 80.0                 svm       0.968116      0.931234\n",
      "spambase                 80.0             xgboost       0.981159      0.948969\n"
     ]
    }
   ],
   "source": [
    "# Main loop \n",
    "\n",
    "def run_experiments():\n",
    "    results = []\n",
    "\n",
    "    # Datasets loop: \n",
    "    for data_name, data_id in datasets_config.items():\n",
    "        x, y = load_and_prep_data(data_name, data_id)\n",
    "        if x is None: continue\n",
    "\n",
    "        print(f\"  dataset shape: {x.shape}\")\n",
    "        \n",
    "        # Partitions loop (training size ratios)\n",
    "        for train_size in partitions:\n",
    "            test_size = 1.0 - train_size\n",
    "            print(f\"\\n  --- partition: train {int(train_size*100)}% / test {int(test_size*100)}% ---\")\n",
    "            \n",
    "            # Classifiers loop\n",
    "            for model_name in model_param_grids.keys():\n",
    "                print(f\"    running {model_name}...\")\n",
    "                \n",
    "                trial_scores_test = []\n",
    "                trial_scores_train = []\n",
    "                \n",
    "                # Trials loop (repeat 3 times)\n",
    "                for trial in range(num_trials):\n",
    "                    # 1. Split data\n",
    "                    x_train, x_test, y_train, y_test = train_test_split(\n",
    "                        x, y, train_size=train_size, random_state=42+trial, stratify=y\n",
    "                    )\n",
    "                    \n",
    "                    # 2. Setup pipeline\n",
    "                    pipeline = get_pipeline(model_name, x_train)\n",
    "                    \n",
    "                    # 3. Hyperparameter tuning (cross-validation on train set only)\n",
    "                    param_grid = model_param_grids[model_name]['params']\n",
    "                    search = RandomizedSearchCV(\n",
    "                        pipeline, \n",
    "                        param_distributions=param_grid, \n",
    "                        n_iter=3,\n",
    "                        cv=3, \n",
    "                        scoring='accuracy', \n",
    "                        n_jobs=-1,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    # 4. Train\n",
    "                    search.fit(x_train, y_train)\n",
    "                    \n",
    "                    # 5. Evaluate (best model)\n",
    "                    best_model = search.best_estimator_\n",
    "                    \n",
    "                    # Check train accuracy (sanity check)\n",
    "                    train_pred = best_model.predict(x_train)\n",
    "                    train_acc = accuracy_score(y_train, train_pred)\n",
    "                    \n",
    "                    # Check test accuracy (real metric)\n",
    "                    test_pred = best_model.predict(x_test)\n",
    "                    test_acc = accuracy_score(y_test, test_pred)\n",
    "                    \n",
    "                    trial_scores_train.append(train_acc)\n",
    "                    trial_scores_test.append(test_acc)\n",
    "                \n",
    "                # Average results across trials\n",
    "                avg_train = np.mean(trial_scores_train)\n",
    "                avg_test = np.mean(trial_scores_test)\n",
    "                \n",
    "                print(f\"      -> avg test acc: {avg_test:.4f} | avg train acc: {avg_train:.4f}\")\n",
    "                \n",
    "                # Store for report\n",
    "                results.append({\n",
    "                    'dataset': data_name,\n",
    "                    'partition_train_pct': train_size * 100,\n",
    "                    'classifier': model_name,\n",
    "                    'avg_train_acc': avg_train,\n",
    "                    'avg_test_acc': avg_test,\n",
    "                    'best_params': search.best_params_\n",
    "                })\n",
    "\n",
    "    # Report generation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"final results summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create dataframe for clean display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort for easier reading\n",
    "    results_df = results_df.sort_values(by=['dataset', 'partition_train_pct', 'classifier'])\n",
    "    \n",
    "    print(results_df[['dataset', 'partition_train_pct', 'classifier', 'avg_train_acc', 'avg_test_acc']].to_string(index=False))\n",
    "    \n",
    "    # Save to csv\n",
    "    results_df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "# Execute the function\n",
    "run_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96deae69-1c9b-47d9-88f5-64b939a51687",
   "metadata": {},
   "source": [
    "## Experimental Results Summary: ##\n",
    "The execution of the pipeline yielded definitive performance rankings across our diverse data domains.\n",
    "\n",
    "* **Adult Dataset**: The gradient-boosted ensemble (XGBoost) proved superior, achieving the highest testing accuracy of 86.7% with 80% training data,  performing better than Random Forest (85.8%) and SVM (85.6%). This confirms the strength of boosting on heterogeneous tabular data.\n",
    "  \n",
    "* **Letter Recognition**: The Support Vector Machine (SVM) achieved near-perfect performance (99.9%) with 80% training data, slightly outperforming the tree-based ensembles (99.8%). This highlights the efficacy of kernel methods in geometric/pixel-based feature spaces.\n",
    "\n",
    "* **Spambase**: Random Forest took the lead here with 95.0% accuracy with 80% training data, demonstrating that bagging methods are highly effective for high-dimensional text feature spaces, outperforming the logistic regression baseline (92.1%) by a significant margin.\n",
    "\n",
    "* **Mushroom**: All classifiers achieved effectively 100% accuracy by the 50% training split, with Logistic Regression and Random Forest achieving >99.9% even at the 20% split. This confirms the dataset contains deterministic rules easily captured by all model types.\n",
    "\n",
    "* **General Trend**: Across all classifiers, increasing the training partition from 20% to 80% resulted in consistent accuracy gains, though the \"diminishing returns\" effect was observed, where the leap from 20% to 50% was often larger than from 50% to 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211d507-5bb6-413d-8861-d0d05a16dde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
